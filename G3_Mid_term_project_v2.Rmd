---
title: "2022 Mid term project- Unemployment Insurance descincentivazation to seek employment"
author: "Group_3 Dustin Riles, Nina Ebensperger,Alejandra Mejia"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    always_allow_html: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
    urlcolor: blue
---


```{r init, include=F}
# Housekeeping
#rm(list = ls())


# Packages needed
#tinytex::install_tinytex()
#install.packages("dplyr")      
#install.packages("plyr")       
#install.packages("readr")   
#installed.packages('skimr', repos = "http://cran.us.r-project.org")
#install.packages
#install.packages("panelr")
#install.packages("lubridate")
#install.packages("eeptools")
#install.packages('xtable')
#install.packages('plm')
#install.packages('tseries')
#install.packages('dynlm')
#install.packages('vars')
#install.packages('broom')
#install.packages('stargazer')
#install.packages('lmtests')
#install.packages("kableExtra")
#install.packages('plotly')
#install.packages('GGally')
library(plotly)
library(kableExtra)
library(skimr)
library(panelr)
library(knitr)
library(stargazer)
library(dplyr)
library(readr)
library(lubridate)
library(xtable)
library(plm) 
library(corrplot)
library(tidyverse)
library(tseries) # for `adf.test()`
library(dynlm) #for function `dynlm()`
library(vars) # for function `VAR()`
library(lmtest) #for `coeftest()` and `bptest()`.
library(broom) #for `glance(`) and `tidy()`
library(ezids)
library(plot3D)
library(tinytex)
library(GGally, quietly = TRUE)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	results = "hide"
)

options(scientific=T, digits = 3) 
```

# The Pandemic and the Unemployment Insurance
## The beginning


[Edition requiered]In the past 20 years, the world has experienced two great economic catastrophes. The first of those, The Great Recession of 2008, was caused by a myriad of factors, but at the center of it all was a failure of
institutions. The 2020 recession caused by the COVID-19 Pandemic was of a different nature. The world put the economy on hold in order to protect global health. Regardless of the cause, ordinary citizens felt the impact of each recession. One of the main ways this is seen is through the unemployment rate. In April 2020, the United States reached a previously unseen level of 14.7 percent. While the unemployment rate did not skyrocket quite as high during the Great Recession, there was still a sizeable increase. During both recessions, the federal government authorized enhanced and extended unemployment insurance benefits.

While necessary for many to keep afloat during the economic crises, a potential drawback of this policy is that workers may be disincentivized to work. If the benefit level is sufficiently high, returning to work may pay less money than remaining on unemployment insurance. A rational actor would see this and decide to stay on unemployment insurance and not return to the workforce. We seek to explore this topic and examine if workers were disincentivized to return to work. See photo:  
 ![Unemployment Insurance](C:/Users/apmej/OneDrive/Escritorio/R/UI_image.jpeg)

## Why Pooled Cross section data is relevant?







## About the data

The weekly Census Household Pulse Survey, which collects data from an average of around 97,000 respondents per week. The details for the dataset can be found here:
https://www.census.gov/programs-surveys/household-pulse-survey/datasets.html. The main reasons we used this data includes: .........


Let us now merge and use the big data dataset using R.


       
 
         
           
```{r init_data}
#### Cleaning the data####
# Setting working directory
setwd('C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\Data\\')

#import and merge all CSV files into one data frame

df_panel <- list.files(path='C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\Data\\') %>% 
  lapply(read_csv) %>% 
  bind_rows 

# remove additional unnecessary rows
df_panel<- df_panel[ -c(27:50) ]
df_panel<- df_panel[ -c(5:6) ]
df_panel<- df_panel[ -c(1) ]
df_panel<- df_panel[ -c(24:25) ]

#creating the panel

df_panel<- panel_data(df_panel, id = SCRAM, wave = WEEK)

#renaming columns
colnames(df_panel)[1:20]=c("ID","Week","State","Age","Hispanic","Other_race","Edu","Sex","p_hh","wrklossrv", 'anywork','kindwork','rsnnowrkrv','UI_apply', 'UI_recrv','UI_recvnow','house', 'income','SPND1','SPND2','SPND3','SPND4','SPND5' )

#Changing birth year to age
df_panel$Age <- 2022-df_panel$Age

#Changing -99 and -88 for NAs
df_panel <- df_panel %>% dplyr::na_if(-99)
df_panel <- df_panel %>% dplyr::na_if(-88)

#Reconverting to panel data
df_panel2<- panel_data(df_panel, id = ID, wave = Week)
df_panel2

# Changing state names first run 
rep_str_state = c('10'='Delaware',
                  '11'='District of Columbia',
                  '12'='Florida',
                  '13'='Georgia',
                  '15'='Hawaii',
                  '16'='Idaho',
                  '17'='Illinois',
                  '18'='Indiana',
                  '19'='Iowa',
                  '20'='Kansas',
                  '21'='Kentucky',
                  '22'='Louisiana',
                  '23'='Maine',
                  '24'='Maryland',
                  '25'='Massachusetts',
                  '26'='Michigan',
                  '27'='Minnesota',
                  '28'='Mississippi',
                  '29'='Missouri',
                  '30'='Montana',
                  '31'='Nebraska',
                  '32'='Nevada',
                  '33'='New Hampshire',
                  '34'='New Jersey',
                  '35'='New Mexico',
                  '36'='New York',
                  '37'='North Carolina',
                  '38'='North Dakota',
                  '39'='Ohio',
                  '40'='Oklahoma',
                  '41'='Oregon',
                  '42'='Pennsylvania',
                  '44'='Rhode Island',
                  '45'='South Carolina',
                  '46'='South Dakota',
                  '47'='Tennessee',
                  '48'='Texas',
                  '49'='Utah',
                  '50'='Vermont',
                  '51'='Virginia',
                  '53'='Washington',
                  '54'='West Virginia',
                  '55'='Wisconsin',
                  '56'='Wyoming')

df_panel2$State<- str_replace_all(df_panel2$State,rep_str_state)

# Changing state names second run 
rep_str_state2 = c('1'='Alabama',
                  '2'='Alaska',
                  '4'='Arizona',
                  '5'='Arkansas',
                  '6'='California',
                  '8'='Colorado',
                  '9'='Connecticut')

df_panel2$State<- str_replace_all(df_panel2$State,rep_str_state2)

# removing NA's from ID column in dataframe

df_panel2<-df_panel2[!is.na(df_panel2$ID),]
df_panel2<-df_panel2[!is.na(df_panel2$State),]

#Adding mean replacement rate column by state and week

df_panel2$Week2[df_panel2$Week %in% 1:9]<- '1-9'
df_panel2$Week2[df_panel2$Week %in% 10:15]<- '10-15'
df_panel2$Week2[df_panel2$Week %in% 16:21]<- '16-21'
df_panel2$Week2[df_panel2$Week %in% 22:27]<- '22-27'
df_panel2$Week2[df_panel2$Week %in% 28:33]<- '28-33'
df_panel2$Week2[df_panel2$Week %in% 34:38]<- '34-38'
df_panel2$Week2[df_panel2$Week %in% 39:40]<- '39-40'
df_panel2$Week2[df_panel2$Week %in% 41:43]<- '41-43'
df_panel2$Week2[df_panel2$Week %in% 44:49]<- '44-49'

#Importing replacement rate data CSV
setwd('C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\')
rr<- read_csv("Replacement_rate.csv")

#Adding Mean replacement rate to data panel

df_panel3<- left_join(df_panel2, rr, by= c('Week2'='week','State'))
df_panel3

#removing additional unnecessary columns
df_panel3<- as.data.frame(df_panel3)
df_panel3<- df_panel3[ -c(20:21) ]
df_panel3<- df_panel3[ -c(19:22) ]

#copying df2 to df3
df_panel2<-df_panel3

#Changing categorical values for traditional 0 & 1    triple check this with dictionary
df_panel2$Hispanic[df_panel2$Hispanic==2]<- 0
df_panel2$Sex[df_panel2$Sex==2]<- 'Female' #Changing to categorical
df_panel2$Sex[df_panel2$Sex==1]<- 'Male'   #Changing to categorical

df_panel2$Edu[df_panel2$Edu==1]<- 'Less than high school' #Changing to categorical
df_panel2$Edu[df_panel2$Edu==2]<- 'Some high school' #Changing to categorical
df_panel2$Edu[df_panel2$Edu==3]<- 'High school grad or equiv' #Changing to categorical
df_panel2$Edu[df_panel2$Edu==4]<- 'Some college no degree received' #Changing to categorical
df_panel2$Edu[df_panel2$Edu==5]<-"Associate's degree" #Changing to categorical
df_panel2$Edu[df_panel2$Edu==6]<- "Bachelor's degree" #Changing to categorical
df_panel2$Edu[df_panel2$Edu==7]<- 'Graduate degree' #Changing to categorical

df_panel2$income[df_panel2$income==1]<- 'Less than $25,000' #Changing to categorical
df_panel2$income[df_panel2$income==2]<- '$25,000 - $34,999' #Changing to categorical
df_panel2$income[df_panel2$income==3]<- '$35,000 - $49,999' #Changing to categorical
df_panel2$income[df_panel2$income==4]<- '$50,000 - $74,999' #Changing to categorical
df_panel2$income[df_panel2$income==5]<- '$75,000 - $99,999' #Changing to categorical
df_panel2$income[df_panel2$income==6]<- '$100,000 - $149,999' #Changing to categorical
df_panel2$income[df_panel2$income==7]<- '$150,000 - $199,999' #Changing to categorical
df_panel2$income[df_panel2$income==8]<- '$200,000 and above' #Changing to categorical

df_panel2$wrklossrv[df_panel2$wrklossrv==2]<- 0
df_panel2$anywork[df_panel2$anywork==2]<- 0
df_panel2$UI_apply [df_panel2$UI_apply ==2]<- 0
df_panel2$UI_recrv[df_panel2$UI_recrv==2]<- 0
df_panel2$UI_recvnow[df_panel2$UI_recvnow==2]<- 0

#Importing regions data CSV
setwd('C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\')
reg<- read_csv("US_regions.csv")

#Adding regions to data panel

df_panel2<- left_join(df_panel2, reg, by= c('State'))


#Number of people repeated in sample

count_reps<- df_panel2 %>% 
        group_by(ID) %>% 
        summarise(n=n())

N_rep<- count_reps %>%
        filter(n > 1)

#  removing ids column
df_panel2<- df_panel2[ -c(1) ]

#converting data frame to panel
df_panel2<- panel_data(df_panel2, id = State, wave = Week)

#Sub-setting the data sample based to remove persons with an income greater than and that haven't been unemployed    ** seems a drastic change
df_panel2 <- subset(df_panel2, income != 7 & income != 8 & wrklossrv != 0 & anywork != 1)  # LEVE 6 SHOULD BE INCLUDED STILL#





```


# Exploratory Data Analysis (EDA)

Mention if our data set is a panel balanced or unbalanced, the number of unique IDs, the number of variables includes, what type of variables are going to be evaluated and why and the summary statistics


```{r, comment="", prompt=TRUE}
#Checking the type of panel balanced or unbalanced

print('Information about the panel:')
pdim(df_panel2)

# Amount of unique values
print('Amount of unique values:')
length(unique(df_panel2$ID))


#Number of people repeated in sample
print('Number of people repeated in sample: ')
N_rep

#Checking panel structure
#kable(str(df_panel2))
glimpse(df_panel2)
#str(df_panel2)
xkablesummary(df_panel2, title="Summary Statistics",pos='left', bso= 'condensed')
#summary(df_panel2)

```


## What can we see from our data? 

Why histograms don't make sense in Panel data?

Therefore we first need to take a glance on our data to see how a massive amount of information can be relevant and better understood for a regression

```{r, results='asis'}
#** Interviewed sample per sex and state**
  
theme_set(theme_classic())
ss <- ggplot(df_panel2, aes(x=fct_infreq(State)))
ss + geom_bar(aes(fill=Sex), width = 0.7) + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + 
  labs(title="Interviewed sample per sex and state", 
       subtitle="Number of persons")
```

#### Tell something about the graph
```{r, results='asis'}
# **Box plot per age and state**
ggplot(aes(x=Sex, y=Age, fill= Sex), data=subset(df_panel2, !is.na(Sex))) + 
   geom_boxplot() +
  stat_summary(fun=mean, geom="point", shape=4)+ 
  labs(title="Box Plot Age/ Sex")
```


```{r, results='asis'}

e1 <- ggplot(data = df_panel2,
            mapping = aes(x = Region, fill = Edu))
e1 + geom_bar(position = "dodge",
             mapping = aes(y = ..prop.., group = Edu))+ 
              labs(title="Levels of education of the sample",
                   subtitle= 'by US region')


```


```{r, results='asis'}
replacerate1 <- ggplot(df_panel2, aes(x=Region, y = `Average of Sum of Replacement Ratio 2`, fill = Region)) +
  geom_boxplot() +
  stat_summary(fun=mean, geom="point", shape=4) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.y = element_line(color = "lightgray"),
        axis.line = element_line(color = "black"),
        axis.ticks.y=element_blank(),
        axis.ticks.x=element_blank(),
        axis.line.y = element_blank(),
        axis.line.x = element_blank(),
        legend.position = "none") +
  labs(x = "Region", y = "Average of Replacement Ratio", title = "Average of Sum of Replacement Ratio 2", subtitle = "By Region")
replacerate1
```


```{r, results='asis'}
# Make a table of 


###Income data graph (Fig 4.20 https://walker-data.com/census-r/exploring-us-census-data-with-visualization.html)
```


```{r, results='asis'}

#t<-ggplot(df_panel2, aes(x = UI_recrv, fill = UI_recrv,  color= State)) +
 #    geom_point(aes(frame= Week))   
#t

#ggplotly(t) 


#UI<- df_panel2 %>%
#   group_by(Week,State,UI_recrv) %>%
#   summarise(n = sum(UI_recrv == 1)) %>%
#   mutate(Freq = n/378123)

#UI <- table(df_panel2$State,df_panel2$UI_recrv )
#UI
#We need something ##

#ggplot(subset(df_panel2, UI_recrv == 1), aes(x=UI_recrv, fill=interaction(State, Week)))+
#geom_bar(position = position_dodge(width=0.4), aes(y = (..count..)/sum(..count..)))

#table(df_panel2$UIrecrv)
#prop.table(table(df_panel2$wrklossrv, df_panel2$UI_recrv))
#df_panel2<-subset(df_panel2, wrklossrv != 0 )

```


```{r, results='asis'}
#***** testing*****
#Filtering unique values in data set
#df_t<- df_panel2[ -c(1) ]
#str(df_t)
#df_t<- panel_data(df_t, id = State, wave = Week)

#df_t<- df_t %>% 
#       group_by(State, Week) %>%
#       mutate( wrk= cumsum(wrklossrv[wrklossrv==1]/n=n()))

#ggplot(df_t, aes(x = wrk, color = State)) + geom_point(aes(frame= Week))
#gap1

#weekly_counts <- df_panel2 %>%
#  count(Week, State)

#t<-weekly_counts %>% 
# ggplot(mapping = aes(x = Week, y = n, color= State)) +
#     geom_point(aes(frame= Week))    #soso
#ggplotly(t)   ## soso



# Sex state

#ss<- as.data.frame(table(df_panel2$Week,df_panel2$State,df_panel2$Sex,useNA = "ifany"))
#colnames(ss)[1:3]=c("Week", "State","Sex")

#ggplot(mapping = aes(x = State, y = freq, color= Sex)) +
#     geom_point(aes(frame= State))    #soso
#ggplotly(t) 


#ggplot(ss, aes(x = Freq, y = State, fill = Sex)) + 
#  geom_col()+ 
#  labs(title = "Participants per State", 
#       y = "State", 
#       x = "Number of persons", 
 #      caption = "Source: Authors based on Pulse Survey") 


#Sex_state<- df_panel2 %>%
#            group_by(Week,State,Sex) %>%
#            summarise(n = n()) %>%
#            mutate(Freq = n/sum(n))



#ss %>% 
#  ggplot(mapping = aes(x = State, y = Freq)) +
#    geom_point(alpha = 0.2, aes(color = Sex)) 

#ggplot(sst, aes(x=weight, color=sex)) +
#  geom_histogram(fill="white")


#sst %>% ggplot(mapping = aes(x = Week, y = n, color= State)) +
#     geom_line()    #soso

#ggplot(sst, aes(x=Sex)) + geom_histogram()




#Weekly_sex_counts <- df_t %>%
 #                     count(Week, State, Sex)

#ggplot(data = Weekly_sex_counts, mapping = aes(x = Week, y = n, color = Sex)) +
#  geom_line() +
#  facet_wrap(facets =  vars(State))



#Wrk_counts <- df_t %>%
#                      count(Week, State, Sex, wrklossrv)

#test <- ggplot(df_t, aes(x = wrklossrv, y = Sex, color = State)) + geom_point(aes(frame= Week))

#ggplotly(test)



#plotdata <- df_t %>%
#  group_by(Week) %>%
#  mutate(
#    change = case_when(
#      wrklossrv != lag(wrklossrv) ~ TRUE,
#      TRUE ~ FALSE
#    ),
#    n_change = cumsum(change))
#plotdata

#ggplot(plotdata, 
#       aes(x = Week, 
#           y = change)) +
#geom_point()


#df_t <- subset(df_panel3, wrklossrv != 2 & UI_recrv  != 2)  

#df_t<- df_t[ -c(1,4:9,11:14,16:20)]


#df_t2<- df_t %>%
#  group_by(State,Week) %>%
#  summarize(total_wrkloss = sum(wrklossrv), total_ui = sum(UI_recrv))                                                    

#count_reps<- df_panel3 %>% 
#        group_by(Week, State) %>% summarise(wrk= sum(wrklossrv,na.rm = T))

                  


#ggplot(df_t2, aes(x=Week,fill=total_wrkloss)) + geom_histogram() + facet_wrap(~State)

#ggplot(df_t, 
#       aes(x = State, 
#           y =wrklossr)) +
#  geom_point())

#ggplot(df_t, 
#       aes(x = Week, 
#           y = wrklossrv)) +
#  geom_point()
#test<- df_t %>% group_by(State, Week) %>% summarize(wrk= sum(wrklossrv,na.rm = T)) 
#ggplot(test, 
#       aes(x = Week, 
#           y = wrk)) +
#  geom_point()

#empl<-as.factor(df_t$anywork)
#dplyr::count(df_t$anywork, vec) 


#df_tot <- df_t %>%
#  group_by(Week,State) %>%
#    summarize(wk_change = sum(anywork == 1))

#ggplot(df_tot, 
#       aes(x = State, 
#           y =wk_change)) +
#  geom_point()



#***** testing*****


#***** testing*****
#Filtering unique values in data set
#df_t<- df_panel2[ -c(1) ]
#str(df_t)
#df_t<- panel_data(df_t, id = State, wave = Week)

#df_t<- df_t %>% 
#       group_by(State, Week) %>%
#       mutate( wrk= cumsum(wrklossrv[wrklossrv==1]/n=n()))

#ggplot(df_t, aes(x = wrk, color = State)) + geom_point(aes(frame= Week))
#gap1

#weekly_counts <- df_panel2 %>%
#  count(Week, State)

#t<-weekly_counts %>% 
#ggplot(mapping = aes(x = Week, y = n, color= State)) +
#     geom_point(aes(frame= Week))    #soso
#ggplotly(t)   ## soso

#***** testing*****






# GGPLOTS relevant i.e (difference btw sexs, states, UI, other relevant)


#df_panel2<-dfpanel_2%>%mutate(Replacement_rate = case_when(
#  Age) & Pclass==1 ~ 40,
#  is.na(Age) & Pclass==2 ~ 30,
#  is.na(Age) & Pclass==3 ~ 25,
#  TRUE~Age
#  ))
#mutate

#*** Pending****#
#*
#*
#*
#ggplot(df_panel2, aes(x=Week, col=Sex)) + geom_density()
#ggplot(df_panel2, aes(x=Week,fill=Sex)) + geom_histogram() + facet_wrap(~Sex)
#ggplot(df_panel2, aes(x=Week,fill=Sex)) + geom_histogram() + facet_wrap(~Sex, ncol=1)
#*
#df_panel3<- df_panel2 %>% ungroup() 


#test<- df_panel2 %>% group_by(State, Week) %>% summarize(wrk= sum(wrklossrv,na.rm = T)) 
#ggplot(test, 
#       aes(x = Week, 
#           y = wrk)) +
#  geom_point()


#plotdata <- df_panel2 %>%
#  group_by(ID) %>%
#  mutate(
#    change = case_when(
#      wrklossrv != lag(wrklossrv) ~ TRUE,
#      TRUE ~ FALSE
#    ),
#    n_change = cumsum(change)
#     )
#plotdata

#ggplot(plotdata, 
#       aes(x = Week, 
#           y = wrklossrv)) +
#  geom_point()

#mosaicplot(counts, xlab='Week', ylab='wrklossrv',main='Wins by Team', col='orange')



## GGplots btw variables (ID and state, state and sex, state and UI, employment change)


#install.packages("sf")
#library(sf)
#Loading libraries



```


### Plot 
Dynamic scatter ggplotly

```{r , results='asis'}
#df_panel2<- df_panel2 %>% group_by(Week, State)%>% summarise(employment = sum(anywork))
#df_t %>%group_by(Week, State) %>%
#        tally()

#df_t %>% group_by(Week, State) %>%count(anywork,UI_recrv)
#test <- ggplot(df_panel2, 
#               aes(x = Age, y = State, color = State,frame = 'Week')) + 
#               geom_point()
#ggplotly(test)

#df_t <- as.data.frame(df_t)
#df_t <- subset(df_panel2, Week != 1 & Week != 2 & Week != 3)  
#wk_count<- df_t %>% count(Week,anywork)
#df_t2<- df_t %>% ungroup()
#empl<- df_t %>% group_by(Week) %>% summarise(e= sum(anywork,na.rm = T))
#ui_in<- df_t %>% group_by(Week) %>% summarise(e= sum(UI_recrv,na.rm = T))
#count the number of occurances 

# be able to 
#df_tot <- df_t %>%
#  group_by(Week) %>%
#  mutate(wk_change = ifelse(anywork == lag(anywork) | is.na(lag(anywork)), 0, 1),
#         UI_change = ifelse(UI_recrv == lag(UI_recrv) | is.na(lag(UI_recrv)), 0, 1)) %>%
#  summarize(tot_wk = sum(wk_change),
#            tot_ui = sum(UI_change))

#test <-df_tot %>% 
# group_by(Week) %>%
#  summarise(n = n()) %>% ggplot(df_t2, aes(x = anywork, y = UI_recrv, color = State)) + geom_point(aes(frame= Week))
#ggplotly(test)

#colSums(df_tot[3])


```

# Findings
We find that:



# Conclusion
Conclusions???





# Limitations

test test test




# Next Steps


## regresion DId

#estimate the following model on our panel data
#eq <- inv ~ value + capital

#Pooled
pooled = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "pooling")
summary(pooled)

#Fixed effects
fixed = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "within")
summary(fixed)

#Random Effects
random = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "random")
summary(random)

#choosing btw fixed to random
phtest(fixed, random)

#Testing if we require random or fix effect
phtest(fixed, random)

is.data.frame(df_panel)
dim(df_panel)


**Anything else you consider relevant to include latter on...**



# References 

APA Style preferred
databases
papers

# Annex

## Data dictionary

+----------+-------+--------------------------------------------+
|Variable  | Codes | Description        |
+==========+=======+============================================+
|Hispanic  |   0   | Hispanic|
|                              |
|          |   1   | Non Hispanic|    

+----------+-------+--------------------------------------------+
|Other Race |   1   | White          |  
|           |   2   | Black          |
|           |   3   | Asian          |
|           |   4   | Any other race |

+----------+-------+--------------------------------------------+
|Education  |   1   | Less than high school|  
|           |   2   | Some high school  |
|           |   3   | High school grad or equiv  |
|           |   4   | Some college no degree received|   
|           |   5   | Associate's degree|  
|           |   6   | Bachelor's degree |  
|           |   7   | Graduate degree |

+----------+-------+---------------------------------------------+
|Sex        |   0   | Female            |
|           |   1   | Male              |

+----------+-------+---------------------------------------------+
|p_hh       | 1-40  | Number of people in household|

+----------+-------+---------------------------------------------+
|wrklossrv  |       | Recent household job loss|
|           |   0   | No  |
|           |   1   | Yes|
           
+----------+-------+---------------------------------------------+           
|anywork    |       | Employment status for last 7 days|
|           |   0   | No  |
|           |   1   | Yes|

+----------+-------+---------------------------------------------+           
|kindwork   |       | Sector of Employment |
|           |   1   | Government  |
|           |   2   | Private company|
|           |   3   | Non profit |
|           |   4   | Self-employed|
|           |   5   | Working in family business  |

+----------+------+----------------------------------------------+           
|rsnnowrkrv |      | Main reason for not working |
|           |  1   | Didn't want to be employed  |
|           |  2   | Sick or caring for COVID |
|           |  3   | Caring for children |
|           |  4   | Caring for an elderly |
|           |  5   | Concerned getting/spreading COVID |
|           |  6   | Sick (no COVID)or disabled |
|           |  7   | Retired |
|           |  8   | Laid off due to COVID |
|           |  9   | Employer closed temporarily due to COVID |
|           |  10  | Employer went out of business due to COVID |
|           |  11  | Did not have transportation to work  |
|           |  12  | Other reason  |

+----------+-------+--------------------------------------------+
|UI_apply   |       | UI Apply|
|           |   0   | No  |
|           |   1   | Yes  |                           

+----------+-------+--------------------------------------------+
|UI_recrv   |       | UI Receive|
|           |   0   | No  |
|           |   1   | Yes  |           

+----------+-------+--------------------------------------------+
|UI_recvnow |      |UI Receive now|
|           |  0   |No  |
|           |  1   |Yes  |                                

+----------+-------+--------------------------------------------+
|house      |       | Housing owned or rented|
|           |   0   | No  |
|           |   1   | Yes  |

+----------+-------+--------------------------------------------+
|income     |       | Total household income|
|           |   1   | Less than $25,000  |
|           |   2   | $25,000 - $34,999 |
|           |   3   | $35,000 - $49,999 |
|           |   4   | $50,000 - $74,999|
|           |   5   | $75,000 - $99,999 | 
|           |   6   | $100,000 - $149,999|
|           |   7   | $150,000 - $199,999|
|           |   8   | $200,000 and above|

+----------+-------+--------------------------------------------+
|Mean RR 1  |       | Replacement Ratio 1=Weighted Avg of:WBA/(Normal Hourly Wage x 40 Hrs.)|

+----------+-------+--------------------------------------------+
|Mean RR 2  |       | Replacement Ratio 2=Ratio of: Weighted Avg WBA/Weighted Avg (Normal Hourly Wage x 40| Hrs.)


Add regions table!!!



