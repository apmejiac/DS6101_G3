---
title: "2022 Mid term project- Unemployment Insurance descincentivazation to seek employment"
author: "Group_3 Dustin Riles, Nina Ebensperger,Alejandra Mejia"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    always_allow_html: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
    urlcolor: blue
---


```{r init, include=F}
# Housekeeping
#rm(list = ls())


# Packages needed
#tinytex::install_tinytex()
#install.packages("dplyr")      
#install.packages("plyr")       
#install.packages("readr")   
#installed.packages('skimr', repos = "http://cran.us.r-project.org")
#install.packages
#install.packages("panelr")
#install.packages("lubridate")
#install.packages("eeptools")
#install.packages('xtable')
#install.packages('plm')
#install.packages('tseries')
#install.packages('dynlm')
#install.packages('vars')
#install.packages('broom')
#install.packages('stargazer')
#install.packages('lmtests')
#install.packages("kableExtra")
#install.packages('plotly')
library(plotly)
library(kableExtra)
library(skimr)
library(panelr)
library(knitr)
library(stargazer)
library(dplyr)
library(readr)
library(lubridate)
library(xtable)
library(plm) 
library(corrplot)
library(tidyverse)
library(tseries) # for `adf.test()`
library(dynlm) #for function `dynlm()`
library(vars) # for function `VAR()`
library(lmtest) #for `coeftest()` and `bptest()`.
library(broom) #for `glance(`) and `tidy()`
library(ezids)
library(plot3D)
library(tinytex)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	results = "hide"
)

options(scientific=T, digits = 3) 
```

# The Pandemic and the Unemployment Insurance
## The beginning


[Edition requiered]In the past 20 years, the world has experienced two great economic catastrophes. The first of those, The Great Recession of 2008, was caused by a myriad of factors, but at the center of it all was a failure of
institutions. The 2020 recession caused by the COVID-19 Pandemic was of a different nature. The world put the economy on hold in order to protect global health. Regardless of the cause, ordinary citizens felt the impact of each recession. One of the main ways this is seen is through the unemployment rate. In April 2020, the United States reached a previously unseen level of 14.7 percent. While the unemployment rate did not skyrocket quite as high during the Great Recession, there was still a sizeable increase. During both recessions, the federal government authorized enhanced and extended unemployment insurance benefits.

While necessary for many to keep afloat during the economic crises, a potential drawback of this policy is that workers may be disincentivized to work. If the benefit level is sufficiently high, returning to work may pay less money than remaining on unemployment insurance. A rational actor would see this and decide to stay on unemployment insurance and not return to the workforce. We seek to explore this topic and examine if workers were disincentivized to return to work. See photo:  
 ![Unemployment Insurance](C:/Users/apmej/OneDrive/Escritorio/R/UI_image.jpeg)

## Why Pooled Cross section data is relevant?







## About the data

The weekly Census Household Pulse Survey, which collects data from an average of around 97,000 respondents per week. The details for the dataset can be found here:
https://www.census.gov/programs-surveys/household-pulse-survey/datasets.html. The main reasons we used this data includes: .........


Let us now merge and use the big data dataset using R.







```{r init_data}
#### Cleaning the data####
# Setting working directory
setwd('C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\Data\\')

#import and merge all CSV files into one data frame

df_panel <- list.files(path='C:\\Users\\apmej\\OneDrive\\Escritorio\\R\\Data\\') %>% 
  lapply(read_csv) %>% 
  bind_rows 

# remove additional unnecessary rows
#df_panel<- df_panel[ -c(27:50) ]
df_panel<- df_panel[ -c(26:51) ]
#df_panel<- df_panel[ -c(5:6) ]
df_panel<- df_panel[ -c(4:5) ]
#df_panel<- df_panel[ -c(1) ] we can't remove the first column, it has scram
#df_panel<- df_panel[ -c(24:25) ] removed SPND 4 and SPND 5

#creating the panel

df_panel<- panel_data(df_panel, id = SCRAM, wave = WEEK)

#renaming columns
colnames(df_panel)[1:20]=c("ID","Week","State","Age","Hispanic","Other_race","Edu","Sex","p_hh","wrklossrv", 'anywork','kindwork','rsnnowrkrv','UI_apply', 'UI_recrv','UI_recvnow','house', 'income','SPND1','SPND2','SPND3','SPND4','SPND5' )

#Changing birth year to age
df_panel$Age <- 2022-df_panel$Age

#Changing -99 and -88 for NAs
df_panel <- df_panel %>% dplyr::na_if(-99)
df_panel <- df_panel %>% dplyr::na_if(-88)

#Reconverting to panel data
df_panel2<- panel_data(df_panel, id = ID, wave = Week)
df_panel2

# Changing state names first run 
rep_str_state = c('10'='Delaware',
                  '11'='District of Columbia',
                  '12'='Florida',
                  '13'='Georgia',
                  '15'='Hawaii',
                  '16'='Idaho',
                  '17'='Illinois',
                  '18'='Indiana',
                  '19'='Iowa',
                  '20'='Kansas',
                  '21'='Kentucky',
                  '22'='Louisiana',
                  '23'='Maine',
                  '24'='Maryland',
                  '25'='Massachusetts',
                  '26'='Michigan',
                  '27'='Minnesota',
                  '28'='Mississippi',
                  '29'='Missouri',
                  '30'='Montana',
                  '31'='Nebraska',
                  '32'='Nevada',
                  '33'='New Hampshire',
                  '34'='New Jersey',
                  '35'='New Mexico',
                  '36'='New York',
                  '37'='North Carolina',
                  '38'='North Dakota',
                  '39'='Ohio',
                  '40'='Oklahoma',
                  '41'='Oregon',
                  '42'='Pennsylvania',
                  '44'='Rhode Island',
                  '45'='South Carolina',
                  '46'='South Dakota',
                  '47'='Tennessee',
                  '48'='Texas',
                  '49'='Utah',
                  '50'='Vermont',
                  '51'='Virginia',
                  '53'='Washington',
                  '54'='West Virginia',
                  '55'='Wisconsin',
                  '56'='Wyoming')

df_panel2$State<- str_replace_all(df_panel2$State,rep_str_state)

# Changing state names second run 
rep_str_state2 = c('1'='Alabama',
                  '2'='Alaska',
                  '4'='Arizona',
                  '5'='Arkansas',
                  '6'='California',
                  '8'='Colorado',
                  '9'='Connecticut')

df_panel2$State<- str_replace_all(df_panel2$State,rep_str_state2)

# removing NA's from ID column in dataframe

df_panel2<-df_panel2[!is.na(df_panel2$ID),]

#Adding mean replacement rate column by state and week

df_panel2$Week2[df_panel2$Week %in% 1:9]<- '1-9'
df_panel2$Week2[df_panel2$Week %in% 10:15]<- '10-15'
df_panel2$Week2[df_panel2$Week %in% 16:21]<- '16-21'
df_panel2$Week2[df_panel2$Week %in% 22:27]<- '22-27'
df_panel2$Week2[df_panel2$Week %in% 28:33]<- '28-33'
df_panel2$Week2[df_panel2$Week %in% 34:38]<- '34-38'
df_panel2$Week2[df_panel2$Week %in% 39:40]<- '39-40'
df_panel2$Week2[df_panel2$Week %in% 41:43]<- '41-43'
df_panel2$Week2[df_panel2$Week %in% 44:49]<- '44-49'

#Importing replacement rate data CSV

r<- read_csv("Replacement_rate.csv")

#Adding Mean replacement rate to data panel

df_panel3<- left_join(df_panel2, r, by= c('Week2'='Week','State')) 
df_panel3

#removing additional unnecessary columns
df_panel3<- as.data.frame(df_panel3)
df_panel3<- df_panel3[ -c(20:21) ]
df_panel3<- df_panel3[ -c(19:21) ] # changed from 22 unless you wanted to removed week2?

#copying df2 to df3
df_panel2<-df_panel3

#Changing categorical values for traditional 0 & 1
df_panel2$Hispanic[df_panel2$Hispanic==2]<- 0
df_panel2$Sex[df_panel2$Sex==2]<- 0
df_panel2$wrklossrv[df_panel2$wrklossrv==2]<- 0
df_panel2$anywork[df_panel2$anywork==2]<- 0
df_panel2$UI_apply [df_panel2$UI_apply ==2]<- 0
df_panel2$UI_recrv[df_panel2$UI_recrv==2]<- 0
df_panel2$UI_recvnow[df_panel2$UI_recvnow==2]<- 0

#converting data frame to panel
df_panel2<- panel_data(df_panel2, id = ID, wave = Week)



#Sub-setting the data sample based to remove persons with an income greater than      and that haven't been unemployed

df_panel2 <- subset(df_panel2, income != 6 & income != 7 & income != 8 & wrklossrv != 2 & anywork != 2) 
df_panel2



```


## Exploratory Data Analysis (EDA)

Mention if our data set is a panel balanced or unbalanced, the number of unique IDs, the number of variables includes, what type of variables are going to be evaluated and why and the summary statistics


```{r, results='asis'}
#Checking the type of panel balanced or unbalanced
pdim(df_panel2)

#Number of people repeated in sample
count_reps<- df_panel2 %>% 
        group_by(ID) %>% 
        summarise(n=n())

N_rep<- count_reps %>%
        filter(n > 1)

print(N_rep)

# Amount of unique values
length(unique(df_panel2$ID))

#Checking panel structure
str(df_panel2)

#Checking Summary statistics of the panel 
##**pending remove final hist of result (Ale)

summary(df_panel2)
#descriptive_stats<- summary(df_panel2)
#descriptive_stats%>%
#  kable() %>%
#  kable_styling()
```


## What can we see from our data? 

WHy histograms don't make sense in Panel data?

Therefore we first need to take a glance on our data to see how a massive amount of information can be relevant and better understood for a regression

```{r, results='asis'}
# GGPLOTS relevant i.e (difference btw sexs, states, UI, other relevant)


#df_panel2<-dfpanel_2%>%mutate(Replacement_rate = case_when(
#  Age) & Pclass==1 ~ 40,
#  is.na(Age) & Pclass==2 ~ 30,
#  is.na(Age) & Pclass==3 ~ 25,
#  TRUE~Age
#  ))
#mutate

#*** Pending****#
df_panel3<- df_panel2 %>% ungroup() 



str(df_panel3)
test<- df_panel3 %>% group_by(State, Week) %>% summarize(wrk= sum(wrklossrv,na.rm = T)) 
ggplot(test, 
       aes(x = Week, 
           y = wrk)) +
  geom_point()


plotdata <- df_panel2 %>%
  group_by(ID) %>%
  mutate(
    change = case_when(
      wrklossrv != lag(wrklossrv) ~ TRUE,
      TRUE ~ FALSE
    ),
    n_change = cumsum(change)
     )
plotdata

ggplot(plotdata, 
       aes(x = Week, 
           y = wrklossrv)) +
  geom_point()

mosaicplot(counts, xlab='Week', ylab='wrklossrv',main='Wins by Team', col='orange')



## GGplots btw variables (ID and state, state and sex, state and UI, employment change)





```


### Plot 
Dynamic scatter ggplotly

```{r , results='asis'}
df_panel2<- df_panel2 %>% group_by(Week, State)%>% summarise(employment = sum(anywork))



#test <- ggplot(df_panel2, 
#               aes(x = Age, y = State, color = State,frame = 'Week')) + 
               geom_point()
#ggplotly(test)

```

### Findings
We find that:



## Conclusion
Conclusions???





##Limitations

test test test




## Next Steps


## regresion DId

#estimate the following model on our panel data
#eq <- inv ~ value + capital

#Pooled
pooled = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "pooling")
summary(pooled)

#Fixed effects
fixed = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "within")
summary(fixed)

#Random Effects
random = plm(eq, data = df_panel2, index = c("ID", "Week"), model = "random")
summary(random)

#choosing btw fixed to random
phtest(fixed, random)

#Testing if we require random or fix effect
phtest(fixed, random)

is.data.frame(df_panel)
dim(df_panel)


**Anything else you consider relevant to include latter on...**



## Reference 

APA Style preferred
databases
papers

## Annex
Dictionary as a table
